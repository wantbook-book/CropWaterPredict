## tranformer 注意

### 存在问题

1. 标准Transformer的自注意机制（Self-Attention）的计算复杂度和内存需求随序列长度线性增加，这限制了其在长序列上的应用；
2. 长时间序列可能含有复杂的时间依赖性，这要求模型能够捕捉到这些长期依赖。

为了解决这些问题，可以采用以下几种策略：

### 1. 使用稀疏注意力机制

稀疏注意力机制（如BigBird、Longformer）在保持Transformer优势的同时，通过限制自注意力的范围来降低计算复杂度，使模型能够处理更长的序列。这些模型通过仅在序列的局部区域内或按某种模式（如滑动窗口）计算注意力权重，来减少计算负担。

### 2. 利用卷积层或池化层

在Transformer模型之前引入卷积层或池化层（降采样）可以减少序列长度，这有助于模型捕捉局部特征，并减少后续层的计算负担。这种方法通常用于序列数据的预处理阶段。

### 3. 分块处理

将长时间序列分成多个较短的序列块，并在每个块上独立运行Transformer模型。这要求在模型设计时考虑如何在块之间传递信息，以保持序列的全局上下文。可以在最后一个Transformer块的输出上应用聚合操作（如平均池化），以获得整个序列的表示。

### 4. 使用时间嵌入

对于时间序列数据，除了输入的数值特征外，还可以引入时间嵌入来捕捉时间的周期性和趋势。例如，可以为序列中的每个时间点生成一个时间戳嵌入，并将其与原始特征一起输入到模型中。

## CNN 网络

将图片数据转换为一维特征。

可使用的 cnn 网络：

1. LeNet
2. AlexNet
3. VGG
4. ResNet
5. Inception
6. DenseNet
7. EfficientNet

使用预训练模型，根据任务调整模型的最后几层



## 图像分割算法

1. 阈值分割

2. 基于区域的阈值分割算法

   区域生长

3. 边缘检测

   canny、sobel、laplacian

4. 轮廓检测

5. 分水岭算法

6. 